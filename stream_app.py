import streamlit as st
import pandas as pd
import chromadb
from sentence_transformers import SentenceTransformer
import uuid
import os
from mistralai import Mistral



EMBEDDING_MODEL_NAME = "paraphrase-multilingual-MiniLM-L12-v2"
COLLECTION_NAME = "ma_collection_muffins"


@st.cache_resource
def initialiser_base_donnees():
    
    # Chargement de la base de donn√©es de recette JSON, cr√©√© par le fichier donn√©es_recettes.ipynb
    if not os.path.exists('base_de_donnees.json'):
        st.error("Fichier 'base_de_donnees.json' introuvable sur GitHub !")
        st.stop()
        
    df = pd.read_json('base_de_donnees.json')
    df_copy = df.copy().fillna("")
    
    # Nettoyage des listes pour ChromaDB
    for col in df_copy.columns:
        df_copy[col] = df_copy[col].apply(lambda x: ", ".join(map(str, x)) if isinstance(x, list) else x)
    
    # Cr√©ation du mod√®le d'embedding
    model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    embeddings = model.encode(df_copy["text_for_embedding"].tolist(), normalize_embeddings=True).tolist() # On utilise le text_for_embedding  cr√©√© dans la base de donn√©es

    # Client Chroma 
    client = chromadb.Client()
    
    # Cr√©ation de la collection
    collection = client.create_collection(name=COLLECTION_NAME)
    collection.add(
        documents=df_copy["text_for_embedding"].tolist(),
        embeddings=embeddings,
        metadatas=df_copy.to_dict(orient='records'),
        ids=[str(uuid.uuid4()) for _ in range(len(df_copy))]
    )
    return collection, model



with st.sidebar:
    st.title("Configuration üîë")
    user_api_key = st.text_input("Entre ta cl√© API Mistral :", type="password")
    st.info("Tu peux obtenir une cl√© sur console.mistral.ai")


# G√©n√©ration de la r√©ponse par le Chef Muffin
# G√©n√©ration de texte avec une cl√© Mistral API
def generer_reponse_chef(query, results, api_key):
    if not api_key:
        return "Oups ! Il me manque ta cl√© API dans la barre lat√©rale pour pouvoir cuisiner... üßÅ"
    
    client = Mistral(api_key=api_key) # On utilise la cl√© API Mistral fournie par l'utilisateur
    
    # On construit le contexte √† partir des r√©sultats de ChromaDB
    # Version plus structur√©e pour l'IA
    contexte = ""
    for m in results['metadatas'][0]:
        contexte += f"""
        ---
        RECETTE : {m['titre']}
        INGR√âDIENTS : {m.get('ingredients', 'Non list√©s')}
        INSTRUCTIONS : {m.get('instructions', 'Non pr√©cis√©es')}
        DESCRIPTION : {m.get('description', '')}
        """
    # Instructions pour mon prompt
    prompt = f"""TU ES UNE CHEFFE MUFFIN, UNE ASSISTANTE CULINAIRE OBSESSIONNELLE MAIS SYMPATHIQUE.
TON OBJECTIF EST DE TROUVER LA RECETTE DE MUFFIN ID√âALE PARMI LE CONTEXTE FOURNI.

### TES DIRECTIVES (GUARDRAILS) :
1. OBSESSION : Tu ne cuisines QUE des muffins. Si on te demande des lasagnes ou une pizza, REFUSE poliment avec humour.
2. ANCRAGE : Utilise UNIQUEMENT les recettes fournies dans le bloc [CONTEXTE]. N'invente rien.
3. LANGUE : R√©ponds toujours en fran√ßais courant.
4. CORRECTION : si l'utilisateur te demande de cuisiner avec des choses qui ne sont pas des aliments, r√©ponds lui avec humour que tu n'es pas m√©canicien, ou magicien etc... 
5. Il y a plusieurs cas, si l'utilistaeur te donne des ingr√©dients/√† une requ√™te qui correspond tr√®s bien avec l'une des 3 recettes de results, alors ne renvoit que cette recette √† l'utilisateur,
si les 3 propositions sont proches mais ne correspondent pas exactement, dis √† l'utilisateur que tu n'as pas en stock une recette qui correspond parfaitement √† ses attentes mais propose
lui les trois recettes en suggestions, pour que √ßa l'inspire ! Attention, ces recettes doivent quand m√™me contenir au moins l'un des ingr√©dient demand√©, ou bien √™tre dans la m√™me famille d'aliment :
par exemple si je demande courgettes tu dois proposer au moins un muffin avec un autre l√©gume. Si tu consid√®res que l'une des propositions ne correspond pas, ne la propose pas!

Si les 3 propositions n'ont rien √† voir alors ne rien renvoyer, et demander √† l'utilisateur une requ√™te moins originale. 

Si l'utilisateur te donne des ingr√©dients pour une recette sal√©e, ne lui propose surtout pas les recettes sucr√©es et inversement, il vaut mieux ne rien r√©pondre stp.

### STRUCTURE DE R√âPONSE STRICTE (√Ä RESPECTER LIGNE PAR LIGNE) :
Pour chaque recette, respecte scrupuleusement cet affichage, tu dois renvoyer tels qu'ils sont dans le [CONTEXTE] exactement, le titre, les ingr√©dients et les instructions :

üìç **[TITRE DE LA RECETTE]**



üõí **Ingr√©dients :**
- [Ingr√©dient 1]
- [Ingr√©dient 2]



üë®‚Äçüç≥ **Instructions :** 
[Recopie ici TOUTES les instructions d√©taill√©es fournies dans le contexte, sans rien r√©sumer et en gardant le ton original.]



‚ú® *Le mot de la Cheffe :*
[Ton commentaire humoristique]


Dans tous les cas, r√©ponds toujours avec bonne humeur, entrain et humour ! Tu es une fan inconditionnel de muffins. Ne finis juste pas par une question. 

[CONTEXTE]
{contexte}
[QUESTION]
{query} """
    chat_response = client.chat.complete(
          model="mistral-small-latest", 
          messages=[
              {
                  "role": "user",
                  "content": prompt,
              },
          ]
      )
      
    return chat_response.choices[0].message.content

# Interface utilisateur = Application Streamlit
st.set_page_config(page_title="Cheffe Muffin", page_icon="üßÅ")

st.title("Rag √† muffins üë©üèº‚Äçüç≥")
st.markdown(":rainbow[Bienvenue !] Je suis la cheffe muffin, je poss√®de dans mon grimoire tout un tas de recettes de muffins, plus d√©licieuses les unes que les autres ! Des envies particuli√®res aujourd'hui ? Je vous trouverai LA recette la plus adapt√©e.")

# Initialisation au chargement de la page
with st.spinner("La Cheffe pr√©pare sa cuisine... (Initialisation)"):
    collection, model_embed = initialiser_base_donnees()

# Champ de saisie
query = st.text_input("Quelle envie avez-vous aujourd'hui ?", placeholder="Ex: J'ai tr√®s envie de fromage ce soir")

if st.button("Demander √† la Cheffe"):
    if not user_api_key:
        st.error("N'oubliez pas de saisir votre cl√© API dans la barre lat√©rale ! üëà")

    elif query:
        with st.spinner("Recherche de la meilleure recette dans mon grimoire..."):
            # Recherche vectorielle
            query_vector = model_embed.encode([query], normalize_embeddings=True).tolist()
            res = collection.query(query_embeddings=query_vector, n_results=3)
            
            # Appel √† l'IA
            reponse = generer_reponse_chef(query, res, user_api_key)
            
            # Affichage
            st.chat_message("assistant").write(reponse)
            with st.expander("üîç V√©rifier les sources du grimoire"):
                for m in res['metadatas'][0]:
                    st.write(f"üìñ **{m['titre']}**")
            
    else:
        st.warning("Dites-moi quelque chose, je ne lis pas encore dans les pens√©es ! üßÅ")
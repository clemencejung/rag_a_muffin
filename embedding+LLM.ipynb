{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c9a0f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] La procÃ©dure spÃ©cifiÃ©e est introuvable'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\miniconda\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "# --- CONFIGURATION FRANÃ‡AISE ðŸ‡«ðŸ‡· ---\n",
    "EMBEDDING_MODEL_NAME = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "COLLECTION_NAME = \"royaume_du_muffin_6\"\n",
    "FICHIER_JSON = \"base_de_donnees.json\"\n",
    "\n",
    "def load_and_simulate_data():\n",
    "    \n",
    "    with open(FICHIER_JSON, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def create_embeddings_and_store(df):\n",
    "    print(\"ðŸ¤– Chargement du modÃ¨le d'embedding multilingue...\")\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    df_copy = df_copy.fillna(\"\")\n",
    "    \n",
    "    # On transforme la colonne 'ingredients' (liste) en texte (str)\n",
    "    # On vÃ©rifie d'abord si c'est bien une liste pour Ã©viter de faire planter le code\n",
    "    df_copy['ingredients'] = df_copy['ingredients'].apply(\n",
    "        lambda x: \", \".join(x) if isinstance(x, list) else x\n",
    "    )\n",
    "    # ConcatÃ©nation Titre + IngrÃ©dients pour la recherche\n",
    "    documents = df_copy[\"text_for_embedding\"].tolist()\n",
    "    metadatas = df_copy.to_dict(orient='records')\n",
    "    ids = [str(uuid.uuid4()) for _ in range(len(df_copy))]\n",
    "\n",
    "    print(\"âš¡ Vectorisation en cours...\")\n",
    "    embeddings = model.encode(documents).tolist()\n",
    "\n",
    "\n",
    "    # Stockage ChromaDB\n",
    "    client = chromadb.Client() # En mÃ©moire pour le test\n",
    "    try: client.delete_collection(name=COLLECTION_NAME)\n",
    "    except: pass\n",
    "\n",
    "    collection = client.create_collection(name=COLLECTION_NAME)\n",
    "    collection.add(documents=documents, embeddings=embeddings, metadatas=metadatas, ids=ids)\n",
    "\n",
    "    print(f\"âœ… Indexation terminÃ©e ! {collection.count()} recettes stockÃ©es.\")\n",
    "    return collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99bb911c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Chargement du modÃ¨le d'embedding multilingue...\n",
      "âš¡ Vectorisation en cours...\n",
      "âœ… Indexation terminÃ©e ! 375 recettes stockÃ©es.\n"
     ]
    }
   ],
   "source": [
    "# --- TEST ---\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_and_simulate_data()\n",
    "    db = create_embeddings_and_store(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bab0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Question: 'Je suis fan de pizza'\n",
      "ðŸ‘‰ Top 1: Pizza faÃ§on muffin\n",
      "ðŸ‘‰ Top 2: Muffins pizzas faciles\n",
      "ðŸ‘‰ Top 3: Muffins 'pizza'\n",
      "ðŸ‘‰ Top 4: Muffins burger Ã  l'italienne\n",
      "ðŸ‘‰ Top 5: Muffin feuilletÃ© tomate aubergine mozzarella\n",
      "ðŸ‘‰ Top 6: Muffins au fromage Ã  raclette\n"
     ]
    }
   ],
   "source": [
    "# Test de recherche en franÃ§ais\n",
    "query = \"Je suis fan de pizza\"\n",
    "model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "results = db.query(query_embeddings=model.encode([query]).tolist(), n_results=6)\n",
    "print(f\"\\nðŸ”Ž Question: '{query}'\")\n",
    "\n",
    "for i, metadata in enumerate(results['metadatas'][0]):\n",
    "    print(f\"ðŸ‘‰ Top {i+1}: {metadata['titre']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "795e4b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparer_contexte(results):\n",
    "    \"\"\"Transforme les rÃ©sultats de ChromaDB en texte lisible par le LLM\"\"\"\n",
    "    context_parts = []\n",
    "    for i in range(len(results['documents'][0])):\n",
    "        titre = results['metadatas'][0][i]['titre']\n",
    "        ingredients = results['metadatas'][0][i]['description'] \n",
    "        instructions = results['metadatas'][0][i]['instructions']\n",
    "        \n",
    "        recette_texte = f\"RECETTE {i+1}: {titre}\\nIngrÃ©dients: {ingredients}\\nInstructions: {instructions}\"\n",
    "        context_parts.append(recette_texte)\n",
    "        \n",
    "    return \"\\n\\n---\\n\\n\".join(context_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a6ee088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def generer_reponse_rag(query, context):\n",
    "    # Le prompt du prof\n",
    "    system_prompt = f\"\"\"\n",
    "    TU ES \"CHEF MUFFIN\", UN ASSISTANT CULINAIRE OBSESSIONNEL MAIS SYMPATHIQUE.\n",
    "    TON OBJECTIF EST DE TROUVER LA RECETTE DE MUFFIN IDÃ‰ALE PARMI LE CONTEXTE FOURNI.\n",
    "\n",
    "    ### TES DIRECTIVES (GUARDRAILS) :\n",
    "    1. OBSESSION : Tu ne cuisines QUE des muffins. Si on te demande des lasagnes ou une pizza, REFUSE poliment avec humour.\n",
    "    2. ANCRAGE : Utilise UNIQUEMENT les recettes fournies dans le bloc [CONTEXTE]. N'invente rien.\n",
    "    3. LANGUE : RÃ©ponds toujours en franÃ§ais courant et appÃ©tissant.\n",
    "\n",
    "    [CONTEXTE]\n",
    "    {context}\n",
    "\n",
    "    [QUESTION]\n",
    "    {query}\n",
    "    \"\"\"\n",
    "\n",
    "    # Appel au LLM\n",
    "    response = ollama.chat(model='mistral', messages=[\n",
    "        {'role': 'user', 'content': system_prompt},\n",
    "    ])\n",
    "    \n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb7c764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour google colab\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import uuid\n",
    "from mistralai import Mistral\n",
    "\n",
    "\n",
    "# configuration\n",
    "# MISTRAL_API_KEY = \"u05c6XjGJKcsviOcDlv0OjQSFhg8ztmY\" \n",
    "EMBEDDING_MODEL_NAME = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "COLLECTION_NAME = \"muffin_colab_pro\"\n",
    "\n",
    "# Recherche dans la base de donnÃ©es\n",
    "def create_db_on_colab(df):\n",
    "    df_copy = df.copy().fillna(\"\") \n",
    "    model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    for col in df_copy.columns:\n",
    "        df_copy[col] = df_copy[col].apply(lambda x: \", \".join(map(str, x)) if isinstance(x, list) else x)\n",
    "    \n",
    "    \n",
    "    # Normalisation pour des scores propres\n",
    "    embeddings = model.encode(df_copy[\"text_for_embedding\"].tolist(), normalize_embeddings=True).tolist()\n",
    "    \n",
    "    client = chromadb.Client()\n",
    "    try:\n",
    "        client.delete_collection(name=COLLECTION_NAME)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    collection = client.create_collection(name=COLLECTION_NAME)\n",
    "    \n",
    "    collection.add(\n",
    "        documents=df_copy[\"text_for_embedding\"].tolist(),\n",
    "        embeddings=embeddings,\n",
    "        metadatas=df_copy.to_dict(orient='records'),\n",
    "        ids=[str(uuid.uuid4()) for _ in range(len(df_copy))]\n",
    "    )\n",
    "    return collection, model\n",
    "\n",
    "# GÃ©nÃ©ration de texte avec Mistral API\n",
    "def generer_reponse_chef(query, results):\n",
    "    client = Mistral(api_key=MISTRAL_API_KEY)\n",
    "    \n",
    "    # On construit le contexte Ã  partir des rÃ©sultats de ChromaDB\n",
    "    contexte = \"\\n\".join([f\"- {m['titre']}: {m['description']}\" for m in results['metadatas'][0]])\n",
    "    \n",
    "\n",
    "    # Instructions pour mon prompt\n",
    "    prompt = f\"\"\"TU ES CHEF MUFFIN, UN ASSISTANT CULINAIRE OBSESSIONNEL MAIS SYMPATHIQUE.\n",
    "TON OBJECTIF EST DE TROUVER LA RECETTE DE MUFFIN IDÃ‰ALE PARMI LE CONTEXTE FOURNI.\n",
    "\n",
    "### TES DIRECTIVES (GUARDRAILS) :\n",
    "1. OBSESSION : Tu ne cuisines QUE des muffins. Si on te demande des lasagnes ou une pizza, REFUSE poliment avec humour.\n",
    "2. ANCRAGE : Utilise UNIQUEMENT les recettes fournies dans le bloc [CONTEXTE]. N'invente rien.\n",
    "3. LANGUE : RÃ©ponds toujours en franÃ§ais courant et appÃ©tissant.\n",
    "4. CORRECTION : si l'utilisateur te demande de cuisiner avec des choses qui ne sont pas des aliments, rÃ©ponds lui avec humour que tu n'es pas mÃ©canicien, ou magicien etc... \n",
    "5. Il y a plusieurs cas, si l'utilistaeur te donne des ingrÃ©dients/Ã  une requÃªte qui correspond trÃ¨s bien avec l'une des 3 recettes de results, alors ne renvoit que cette recette Ã  l'utilisateur,\n",
    "si les 3 propositions sont proches mais ne correspondent pas exactement, dis Ã  l'utilisateur que tu n'as pas en stock une recette qui correspond parfaitement Ã  ses attentes mais propose\n",
    "lui les trois recettes en suggestions, pour que Ã§a l'inspire ! Attention, ces recettes doivent quand mÃªme contejnir au moins l'un des ingrÃ©dient demandÃ©, ou bien Ãªtre dans la mÃªme famille d'aliment :\n",
    "par exemple si je demande courgettes il me propose au moins un muffin avec un autre lÃ©gume. Si les 3 propositions n'ont rien Ã  voir alors ne rien renvoyer. \n",
    "Si l'utilisateur te donne des ingrÃ©dients pour une recette salÃ©e, ne lui propose pas les recettes sucrÃ©es.\n",
    "\n",
    "Dans tous les cas, rÃ©ponds toujours avec bonne humeur, entrain et humour ! Tu es un fan inconditionnel de muffins.\n",
    "\n",
    "[CONTEXTE]\n",
    "{contexte}\n",
    "[QUESTION]\n",
    "{query} \"\"\"\n",
    "    chat_response = client.chat.complete(\n",
    "          model=\"mistral-small-latest\", # ModÃ¨le Ã©quilibrÃ© et efficace\n",
    "          messages=[\n",
    "              {\n",
    "                  \"role\": \"user\",\n",
    "                  \"content\": prompt,\n",
    "              },\n",
    "          ]\n",
    "      )\n",
    "      \n",
    "    return chat_response.choices[0].message.content\n",
    "# TEST\n",
    "# Charge la base de donnÃ©es\n",
    "df = pd.read_json('/base_de_donnees.json') \n",
    "\n",
    "# Simulation d'une recherche\n",
    "collection, embedder = create_db_on_colab(df)\n",
    "query = \"j'aime beaucoup la pizza\" # Entrer la requÃªte\n",
    "res = collection.query(query_embeddings=embedder.encode([query], normalize_embeddings=True).tolist(), n_results=3) # On prend les 3 meilleurs rÃ©sultats par ChromaDB\n",
    "print(generer_reponse_chef(query, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347ee917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import random\n",
    "import time\n",
    "\n",
    "st.write(\"Streamlit loves LLMs! ðŸ¤– [Build your own chat app](https://docs.streamlit.io/develop/tutorials/llms/build-conversational-apps) in minutes, then make it powerful by adding images, dataframes, or even input widgets to the chat.\")\n",
    "\n",
    "st.caption(\"Note that this demo app isn't actually connected to any LLMs. Those are expensive ;)\")\n",
    "\n",
    "# Initialize chat history\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"Let's start chatting! ðŸ‘‡\"}]\n",
    "\n",
    "# Display chat messages from history on app rerun\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# Accept user input\n",
    "if prompt := st.chat_input(\"What is up?\"):\n",
    "    # Add user message to chat history\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    # Display user message in chat message container\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    # Display assistant response in chat message container\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        message_placeholder = st.empty()\n",
    "        full_response = \"\"\n",
    "        assistant_response = random.choice(\n",
    "            [\n",
    "                \"Hello there! How can I assist you today?\",\n",
    "                \"Hi, human! Is there anything I can help you with?\",\n",
    "                \"Do you need help?\",\n",
    "            ]\n",
    "        )\n",
    "        # Simulate stream of response with milliseconds delay\n",
    "        for chunk in assistant_response.split():\n",
    "            full_response += chunk + \" \"\n",
    "            time.sleep(0.05)\n",
    "            # Add a blinking cursor to simulate typing\n",
    "            message_placeholder.markdown(full_response + \"â–Œ\")\n",
    "        message_placeholder.markdown(full_response)\n",
    "    # Add assistant response to chat history\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
